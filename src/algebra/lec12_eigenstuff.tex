\begin{definition}
    Niech $f : V \to V$ będzie endomorfizmem. Skalar $\lambda \in \KK$ jest \vocab{wartością własną} tego endomorfizmu, jeśli istnieje niezerowy wektor $\mathbf{v}$ taki, że
    \[ f(\mathbf{v}) = \lambda\mathbf{v}. \]

    Każdy niezerowy wektor $\mathbf{v}$, który spełnia powyższą równość, jest \vocab{wektorem własnym} odpowiadającym wartości własnej $\lambda$.
\end{definition}

\begin{definition}
    Widmo endomorfizmu to zbiór jego wartości własnych.
\end{definition}

\begin{definition}
    Podprzestrzeń własną $V_\lambda$ endomorfizmu $f : V \to V$ tworzy zbiór
    \[ V_\lambda = \{\mathbf{v} \in V : f(\mathbf{v}) = \lambda\mathbf{v}\}, \]
    czyli zbiór wektorów własnych z wektorem zerowym.
\end{definition}
\begin{proof}[Uzasadnienie]
    Fakt, że zbiór $V_\lambda$ tworzy podprzestrzeń liniową przestrzeni $V$, wynika z~addytywności i jednorodności endomorfizmów.
\end{proof}

\begin{theorem}
    Każdy wektor własny odpowiada dokładnie jednej wartości własnej.
\end{theorem}
\begin{proof}
    Załóżmy przeciwnie, że są dwie wartości własne $\lambda_1, \lambda_2$ dla których
    \[ f(\mathbf{v}) = \lambda_1\mathbf{v} = \lambda_2\mathbf{v}. \]
    Wtedy
    \[ \ol{0} = f(\mathbf{v}) - f(\mathbf{v}) = \lambda_1\mathbf{v} - \lambda_2\mathbf{v} = \mathbf{v}(\lambda_1 - \lambda_2). \]
    Skoro $\mathbf{v} \neq \ol{0}$, to $\lambda_1 = \lambda_2$.
\end{proof}

\begin{theorem}
    \label{t:det(A-lambda I)=0}
    Niech $f : V \to V$ będzie endomorfizmem oraz $A = M_f(B)$ dla bazy $B = (e_1, \ldots, e_n)$. Wtedy $\lambda \in \KK$ jest wartością własną wtedy i tylko wtedy, gdy
    \[ \det(A - \lambda I) = 0, \]
    a niezerowy wektor $\mathbf{v} \in V$ jest wektorem własnym odpowiadającym $\lambda$ wtedy i tylko wtedy, gdy
    \[ (A - \lambda I)X = \ol{0}, \]
    gdzie $X$ jest macierzą kolumnową współrzędnych wektora $\mathbf{v}$ w bazie $B$.
\end{theorem}
\begin{proof}
    Mamy
    \[ f(\mathbf{v}) = \lambda\mathbf{v} \iff AX = \lambda X, \]
    co możemy przekształcić do
    \begin{align*}
        AX - \lambda X &= \ol{0} \\
        (A - \lambda I)X &= \ol{0}.
    \end{align*}
    Z twierdzenia Kroneckera-Capellego (\ref{t:Kronecker-Cappelli}) wynika, że powyższy układ równań liniowych ma co najmniej jedno rozwiązanie. A ponieważ jest to układ jednorodny, jednym z rozwiązań na pewno jest $X = \ol{0}$. A więc stwierdzenie, że $\lambda$ jest wartością własną $f$ (to znaczy, że istnieje inne, niezerowe rozwiązanie $X$), jest (na mocy twierdzenia \ref{t:rankA = rankA|B = n}) równoważne temu, że $\rank (A - \lambda I) < n$, a więc $\dim (A - \lambda I) = 0$.
\end{proof}

\begin{definition}
    Wielomian charakterystyczny endomorfizmu $f : V \to V$ to wielomian
    \[ \Delta(f) = \det(A - \lambda I) = (-\lambda)^n + \ldots + \det A, \]
    gdzie $A$ jest macierzą endomorfizmu $f$ w dowolnej bazie.
\end{definition}

\begin{fact}
    \label{f:roots of the characteristic polynomial are eigenvalues}
    Wartości własne endomorfizmu $f$ są wyznaczane przez pierwiastki jego wielomianu charakterystycznego $\Delta(f)$.
\end{fact}
\begin{proof}
    Wynika z twierdzenia \ref{t:det(A-lambda I)=0} oraz twierdzenia Bézouta.
\end{proof}

\begin{lemma}
    \label{l:eigenvectors are linearly independent}
    Wektory własne odpowiadające różnym wartościom własnym danego endomorfizmu są liniowo niezależne.
\end{lemma}
\begin{proof}
    Przeprowadzimy dowód indukcyjny. Jeden niezerowy wektor jest oczywiście liniowo niezależny, więc załóżmy, że zbiór $n-1$ wektorów własnych odpowiadających różnym wartościom własnym jest liniowo niezależny i pokażmy ten fakt również dla $n$ wektorów.

    Oznaczmy wektory własne $\mathbf{v}_1, \ldots, \mathbf{v}_n$. Niech odpowiadają one różnym wartościom własnym $\lambda_1, \ldots, \lambda_n$. Weźmy takie skalary $\alpha_1, \ldots, \alpha_n$, że
    \begin{align}
        \alpha_1\mathbf{v}_1 + \ldots + \alpha_n\mathbf{v}_n &= \ol{0},
            \label{eq:sum alpha_1 v_1 alpha_n v_n} \\
        f(\alpha_1\mathbf{v}_1 + \ldots + \alpha_n\mathbf{v}_n) &= \ol{0}, \nonumber \\
        \alpha_1\lambda_1\mathbf{v}_1 + \ldots + \alpha_n\lambda_n\mathbf{v}_n &= \ol{0}.
            \label{eq:sum alpha_1 lambda_1 v_1 alpha_n lambda_n v_n}
    \end{align}
    Mnożąc równanie \ref{eq:sum alpha_1 v_1 alpha_n v_n} obustronnie przez $\lambda_n$ i odejmując od \ref{eq:sum alpha_1 lambda_1 v_1 alpha_n lambda_n v_n} otrzymamy
    \[ \alpha_1\mathbf{v}_1(\lambda_1 - \lambda_n) + \ldots + \alpha_{n-1}\mathbf{v}_1(\lambda_{n-1} - \lambda_n) = \ol{0}. \]
    Wektory $\mathbf{v}_1, \ldots, \mathbf{v}_{n-1}$ są, na mocy założenia indukcyjnego, liniowo niezależne, więc dla każdego $1 \leq i \leq n-1$ zachodzi
    \[ \alpha_i(\lambda_i - \lambda_n) = 0, \]
    a więc, skoro rozważamy różne wartości własne,
    \[ \alpha_i = 0. \]
    Podstawiając to do równości \ref{eq:sum alpha_1 v_1 alpha_n v_n} otrzymujemy
    \[ \alpha_n\mathbf{v}_n = \ol{0}. \]
    Wektory własne są niezerowe, więc $\alpha_n = 0$, co, na mocy zasady indukcji matematycznej, dowodzi tezy.
\end{proof}

\begin{definition}
    Endomorfizm $f : V \to V$ jest \vocab{diagonalizowalny}, jeśli istnieje baza przestrzeni $V$, w której macierz endomorfizmu $f$ jest diagonalna.
\end{definition}

\begin{theorem}
    \label{t:endomorphism is diagonalizable iff base...}
    Endomorfizm $f: V \to V$, $\dim V < \infty$, jest diagonalizowalny wtedy i tylko wtedy, gdy istnieje baza przestrzeni $V$ utworzona z wektorów własnych endomorfizmu $V$.
\end{theorem}
\begin{proof}
    Weźmy taką bazę $B = (e_1, \ldots, e_n)$, że macierz
    \[ M_f(B) = \begin{bNiceMatrix}
        \lambda_1 & 0 & \Cdots & 0 \\
        0 & \lambda_2 & \Cdots & 0\\
        \Vdots & \Vdots & \Ddots & \Vdots \\
        0 & 0 & \Cdots & \lambda_n \\
    \end{bNiceMatrix} \]
    jest diagonalna. Wtedy dla każdego $1 \leq i \leq n$ zachodzi
    \[ M_f(B) \cdot e_i = f(e_i) = \lambda_i e_i, \]
    więc każdy wektor $e_i$ jest wektorem własnym endomorfizmu $f$.

    Aby uzyskać dowód konieczności, wystarczy powtórzyć powyższy wywód w odwrotnej kolejności --- wszystkie implikacje były równoważnościami.
\end{proof}

\begin{corollary}[z twierdzenia \ref{t:endomorphism is diagonalizable iff base...}]
    \label{c:n different eigenvalues}
    Jeśli endomorfizm $f : V \to V$, ma $n = \dim V$ różnych wartości własnych, to jest diagonalizowalny.
\end{corollary}
\begin{proof}
    Na mocy lematu \ref{l:eigenvectors are linearly independent}, wektory odpowiadające różnym wartościom własnym są liniowo niezależne, więc stanowią bazę, ergo endomorfizm $f$ jest diagonalizowalny. Warto zauważyć, że jeśli endomorfizm nie ma $n$ różnych wartości własnych, to nie znaczy, że nie jest diagonalizowalny.
\end{proof}

\vocab{Krotność algebraiczna} $k_a$ wartości własnej $\lambda$ to jej krotność jako pierwiastka wielomianu charakterystycznego, a jej \vocab{krotność geometryczna} $k_g$ to wymiar przestrzeni własnej $V_\lambda$, czyli $k_g = \dim V_\lambda$.

\begin{lemma}
    Jeżeli $\lambda$ jest wartością własną endomorfizmu $f$ o krotności algebraicznej $k_a$ i geometrycznej $k_g$, to
    \[ 1 \leq k_g \leq k_a, \]
    czyli krotność algebraiczna jest większa lub równa geometrycznej.
\end{lemma}
\begin{proof}
    Pierwsza nierówność wynika wprost z definicji obu krotności.

    Niech $A \in \sM_{n\times n}$ będzie macierzą endomorfizmu $f$, który ma pewną wartość własną $\lambda_0$ o krotności geometrycznej $k_g$. Z twierdzenia \ref{t:det(A-lambda I)=0} mamy, że
    \[ k_g = \dim \Ker(A - \lambda_0 I) = \nullity(A - \lambda_0 I), \]
    a z twierdzenia o rzędzie (\ref{t:rank-nullity}) wynika, że $\rank (A - \lambda_0 I) = n - k_g$. To z kolei znaczy, że możemy wykonać taki ciąg operacji elementarnych, że uzyskamy macierz schodkową $B \sim (A - \lambda_0 I)$, której ostatnie $k_g$ wiersze są zerowe.

    Weźmy pewną zmienną $t$ i wykonajmy identyczny ciąg operacji elementarnych jak poprzednio, otrzymując macierz $C \sim (A - t I)$. Oczywiście, dla $t = \lambda_0$ ostatnie $k_g$ wierszy macierzy $C$ się zeruje, więc $\det C$ jest podzielny przez $(t - \lambda_0)^{k_g}$, ergo $\det (A - \lambda_0 I$) również (różnią ewentualnie o stałą), stąd $k_g \leq k_a$.
\end{proof}

\begin{remark*}[mnemotechnika]
    Krotność algebraiczna (ang. \textit{algebraic multiplicity}) i krotność geometryczna (ang. \textit{geometric multiplicity}) to w skrócie odpowiednio AM i GM. Podobnie średnia arytmetyczna (ang. \textit{arithmetic mean}) i średnia geometryczna (ang. \textit{geometric mean}) to w skrócie odpowiednio AM i GM. W obu przypadkach zachodzi zależność
    \[ \text{AM} \geq \text{GM}. \]
    Jeśli nierówność między średnimi nie jest dla Czytelnika oczywista, warto zobaczyć artykuł na \href{https://en.wikipedia.org/wiki/AM-GM_Inequality}{wikipedii}.
\end{remark*}

\begin{theorem}
    \label{t:endomorphism is diagonalizable iff polynomial...}
    Endomorfizm $f : V \to V$ jest diagonalizowalny wtedy i tylko wtedy, gdy wielomian charakterystyczny jest rozkładalny do postaci
    \[ \Delta(\lambda) = (\lambda_1 - \lambda)^{k_{a1}}\cdots(\lambda_p - \lambda)^{k_{ap}} \]
    oraz dla każdego $i$ zachodzi
    \[ k_{gi} = k_{ai}, \]
    czyli gdy krotności geometryczne i algebraiczne poszczególnych wartości własnych są sobie równe.
\end{theorem}

\begin{remark}
    \label{r:about base of diagonal matrix of endomorphism}
    Jeśli istnieje baza $B$ przestrzeni $V$, w której macierz endomorfizmu $f: V \to V$ jest diagonalna, to $V$ jest sumą (prostą, co wynika z lematu \ref{l:eigenvectors are linearly independent}) przestrzeni własnych, więc $B$ jest sumą mnogościową baz tych przestrzeni.
\end{remark}

\begin{example}
    Sprawdź, czy endomorfizm $f : \RR^3 \to \RR^3$ dany wzorem
    \[ f(x, y, z) = (-x-3z, x + 2y + 3z, -3x - z) \]
    jest diagonalizowalny. Jeśli tak, wyznacz bazę $B$ taką, że $D = M_f(B)$ jest diagonalna, podaj $D$ oraz macierz $P$ taką, że $M_f(B_k) = PDP^{-1}$.
\end{example}
\begin{solution}
    Najpierw wyznaczymy macierz $M_f(B_k)$. Mamy
    \[ f(1, 0, 0) = [-1, 3, -3]_{B_k}, \quad f(0, 1, 0) = [0, 2, 0]_{B_k}, \quad f(0, 0, 1) = [-3, 3, -1]_{B_k}, \]
    tak więc
    \[ M_f(B_k) = \begin{bmatrix}
        -1 & 0 & -3 \\
        3 & 2 & 3 \\
        -3 & 0 & -1
    \end{bmatrix}. \]
    Aby wyznaczyć wartości własne, skorzystamy z faktu \ref{f:roots of the characteristic polynomial are eigenvalues}.
    \begin{align*} \Delta(\lambda) &=
        \begin{vmatrix}
            -1 - \lambda & 0 & -3 \\
            3 & 2 - \lambda & 3 \\
            -3 & 0 & -1 - \lambda
        \end{vmatrix} = (2 - \lambda) \begin{vmatrix}
            -1 - \lambda & -3 \\
            -3 & -1 - \lambda
        \end{vmatrix} \\
        &= (2 - \lambda)((1 + \lambda)^2 - 3^2) = (2 - \lambda)(\lambda + 4)(\lambda - 2) \\
        &= -(2 - \lambda)^2(4 + \lambda)
    \end{align*}
    Tak więc mamy wartości własne $\lambda_1 = 2, \lambda_2 = -4$ z krotnościami odpowiednio $k_{a1} = 2, k_{a2} = 1$. Nie mamy trzech różnych wartości własnych, więc, aby sprawdzić, czy $f$ jest diagonalizowalny, nie możemy zastosować wniosku \ref{c:n different eigenvalues}; będziemy musieli skorzystać z pełnego twierdzenia \ref{t:endomorphism is diagonalizable iff polynomial...}.

    Wektor własny $[a, b, c]$ odpowiada wartości własnej $\lambda_1$ wtw, gdy
    \[ \begin{bmatrix}
        -3 & 0 & -3 \\
        3 & 0 & 3 \\
        -3 & 0 & -3
    \end{bmatrix}\begin{bmatrix}
        a \\ b \\ c
    \end{bmatrix} = \begin{bmatrix}
        0 \\ 0 \\ 0
    \end{bmatrix} \]
    \[ \iff a = -c. \]
    W takim razie
    \[ k_{g1} = \dim V_{\lambda_1} = \dim\Lin\{(1, 0, -1), (0, 1, 0)\} = 2. \]

    Podobnie dla wartości własnej $\lambda_2$ mamy
    \[ \begin{bmatrix}
        3 & 0 & -3 \\
        3 & 6 & 3 \\
        -3 & 0 & 3
    \end{bmatrix}\begin{bmatrix}
        a \\ b \\ c
    \end{bmatrix} = \begin{bmatrix}
        0 \\ 0 \\ 0
    \end{bmatrix} \]
    \[ \iff a = c, a + 2b + c = 0 \iff a = c, a = -b. \]
    A więc
    \[ k_{g2} = \dim V_{\lambda_2} = \dim\Lin\{(1, -1, 1)\} = 1, \]
    z czego wynika, że $f$ jest diagonalizowalny.

    Zgodnie z uwagą \ref{r:about base of diagonal matrix of endomorphism} mamy
    \[ B = ((1, 0, -1), (0, 1, 0), (1, -1, 1)) \]
    oraz
    \[ M_f(B) = \begin{bmatrix}
        2 & 0 & 0 \\
        0 & 2 & 0 \\
        0 & 0 & -4
    \end{bmatrix}. \]
    Musimy wiec jeszcze znaleźć macierz $P = P_{B_k\to B}$:
    \[ P = \begin{bmatrix}
        1 & 0 & 1 \\
        0 & 1 & -1 \\
        -1 & 0 & 1
    \end{bmatrix}. \]
\end{solution}

Możemy również wykorzystać własności macierzy diagonalnych bez potrzeby odwoływania się do endomorfizmów. Na podstawie definicji \ref{d:matrix equivalence} i \ref{d:matrix similarity} będziemy mówili, że macierz jest diagonalizowalna, jeśli jest podobna do pewnej macierzy diagonalnej; że ma wartości własne oraz wektory własne (gdy spełnia równania z twierdzenia \ref{t:det(A-lambda I)=0}) i~tym podobne. Wszystkie wyprowadzone dla endomorfizmów twierdzenia będą również prawdziwe dla macierzy.

\begin{example}
    Niech $A = \begin{bmatrix}
        1 & -3 \\
        1 & 5
    \end{bmatrix}$. Oblicz $A^n$.
\end{example}
\begin{solution}
    \[ \Delta(\lambda) = \begin{vmatrix}
        1 - \lambda & -3 \\
        1 & 5 - \lambda
    \end{vmatrix} = (1-\lambda)(5-\lambda) - (-3) = 8 - 6\lambda + \lambda^2 = (2 - \lambda)(4 - \lambda), \]
    a więc $\lambda_1 = 2, \lambda_2 = 4$. Korzystając z wniosku \ref{c:n different eigenvalues}, dostajemy, że macierz $A$ jest diagonalizowalna, więc możemy przedstawić ją jako
    \[ A^n = (PDP^{-1})^n = PD^nP^{-1} \]
    przy
    \[ D^n = \begin{bmatrix}
        2 & 0 \\
        0 & 4
    \end{bmatrix}^n = \begin{bmatrix}
        2^n & 0 \\
        0 & 4^n
    \end{bmatrix}. \]
    Powyższa równość wynika z faktu, że macierz diagonalną potęguje się bardzo łatwo --- wystarczy podnieść wszystkie jej elementy do potęgi --- co nietrudno jest udowodnić.

    Aby znaleźć macierz $P$, najłatwiej będzie potraktować $A$ oraz $D$ jako macierze tego samego endomorfizmu w różnych bazach. Przyjmiemy, że to $A$ jest w bazie kanonicznej i znajdziemy bazę, w której jest $D$. Najpierw znajdźmy bazy przestrzeni własnych:
    \[ \begin{bmatrix}
        -1 & -3 \\
        1 & 3
    \end{bmatrix}\begin{bmatrix}
        a \\ b
    \end{bmatrix} = \begin{bmatrix}
        0 \\ 0
    \end{bmatrix} \]
    \[ \iff a = -3b, \]
    więc $V_{\lambda_1} = \Lin\{(3, -1)\}$ oraz
    \[ \begin{bmatrix}
        -3 & -3 \\
        1 & 1
    \end{bmatrix}\begin{bmatrix}
        a \\ b
    \end{bmatrix} = \begin{bmatrix}
        0 \\ 0
    \end{bmatrix} \]
    \[ \iff a = -b, \]
    więc $V_{\lambda_2} = \Lin\{(1, -1)\}$.

    Mamy już $B = ((3, -1), (1, -1))$ oraz
    \[ P = P_{B_k\to B} = \begin{bmatrix}
        3 & 1 \\
        -1 & -1
    \end{bmatrix}. \]
    Obliczmy jeszcze macierz $P^{-1}$:
    \[ \begin{bNiceArray}{ccIcc}
        3 & 1 & 1 & 0 \\
        -1 & -1 & 0 & 1
    \end{bNiceArray} \sim \begin{bNiceArray}{ccIcc}
        1 & -1 & 1 & 2 \\
        -1 & -1 & 0 & 1
    \end{bNiceArray} \sim \begin{bNiceArray}{ccIcc}
        1 & -1 & 1 & 2 \\
        0 & -2 & 1 & 3
    \end{bNiceArray} \sim \begin{bNiceArray}{ccIcc}
        1 & 0 & \frac{1}{2} & \frac{1}{2} \\
        0 & 1 & -\frac{1}{2} & -\frac{3}{2}
    \end{bNiceArray}, \]
    tak więc
    \[ P^{-1} = \frac{1}{2}\begin{bmatrix}
        1 & 1 \\
        -1 & -3
    \end{bmatrix}. \]
    Mamy już więc ostateczny wynik
    \begin{align*}
        A^n &= \frac{1}{2}\begin{bmatrix}
            3 & 1 \\
            -1 & -1
        \end{bmatrix}\begin{bmatrix}
            2^n & 0 \\
            0 & 4^n
        \end{bmatrix}\begin{bmatrix}
            1 & 1 \\
            -1 & -3
        \end{bmatrix} \\
        &= \frac{1}{2}\begin{bmatrix}
            3\cdot 2^n & 4^n \\
            -2^n & -4^n
        \end{bmatrix}\begin{bmatrix}
            1 & 1 \\
            -1 & -3
        \end{bmatrix} \\
        &= 2^{n-1}\begin{bmatrix}
            3 - 2^n & 3 - 3\cdot 2^n \\
            2^n - 1 & 3 \cdot 2^n - 1
        \end{bmatrix}.
    \end{align*}
\end{solution}